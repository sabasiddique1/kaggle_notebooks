{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('/kaggle/input/playground-series-s6e1/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/playground-series-s6e1/test.csv')\n",
    "submission_df = pd.read_csv('/kaggle/input/playground-series-s6e1/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'exam_score'\n",
    "base_features = [col for col in train_df.columns if col not in [TARGET, 'id']]\n",
    "num_features = ['study_hours', 'class_attendance', 'sleep_hours']\n",
    "\n",
    "def preprocess(df, is_train=True, target_mean=None):\n",
    "    \"\"\"Create features for train/test\"\"\"\n",
    "    df_temp = df.copy()\n",
    "    \n",
    "    # 1. Log transforms on numerical features\n",
    "    for col in num_features:\n",
    "        if col in df_temp.columns:\n",
    "            df_temp[f'log_{col}'] = np.log1p(df_temp[col])\n",
    "    \n",
    "    # 2. Squared features\n",
    "    for col in num_features:\n",
    "        if col in df_temp.columns:\n",
    "            df_temp[f'{col}_sq'] = df_temp[col] ** 2\n",
    "    \n",
    "    # 3. Learned formula feature (from top notebooks)\n",
    "    df_temp['feature_formula'] = (\n",
    "        5.9051154511950499 * df_temp['study_hours'] + \n",
    "        0.34540967058057986 * df_temp['class_attendance'] + \n",
    "        1.423461171860262 * df_temp['sleep_hours'] + 4.7819\n",
    "    )\n",
    "    \n",
    "    # 4. Interaction features (most important)\n",
    "    df_temp['study_attendance_interaction'] = df_temp['study_hours'] * df_temp['class_attendance']\n",
    "    df_temp['study_sleep_interaction'] = df_temp['study_hours'] * df_temp['sleep_hours']\n",
    "    df_temp['attendance_sleep_interaction'] = df_temp['class_attendance'] * df_temp['sleep_hours']\n",
    "    \n",
    "    # 5. Ratio features\n",
    "    df_temp['study_sleep_ratio'] = df_temp['study_hours'] / (df_temp['sleep_hours'] + 1e-8)\n",
    "    df_temp['attendance_sleep_ratio'] = df_temp['class_attendance'] / (df_temp['sleep_hours'] + 1e-8)\n",
    "    \n",
    "    # 6. Convert categorical to string for XGBoost categorical support\n",
    "    for col in base_features:\n",
    "        if df_temp[col].dtype == 'object':\n",
    "            df_temp[col] = df_temp[col].astype(str)\n",
    "    \n",
    "    # Select features\n",
    "    log_cols = [f'log_{col}' for col in num_features]\n",
    "    sq_cols = [f'{col}_sq' for col in num_features]\n",
    "    interaction_cols = ['study_attendance_interaction', 'study_sleep_interaction', \n",
    "                        'attendance_sleep_interaction', 'study_sleep_ratio', 'attendance_sleep_ratio']\n",
    "    \n",
    "    feature_cols = base_features + log_cols + sq_cols + ['feature_formula'] + interaction_cols\n",
    "    \n",
    "    return df_temp[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "X_raw = preprocess(train_df)\n",
    "y = train_df[TARGET].reset_index(drop=True)\n",
    "X_test_raw = preprocess(test_df)\n",
    "\n",
    "# Combine for categorical encoding\n",
    "full_data = pd.concat([X_raw, X_test_raw], axis=0)\n",
    "\n",
    "# Convert categorical columns\n",
    "for col in base_features:\n",
    "    if full_data[col].dtype == 'object':\n",
    "        full_data[col] = full_data[col].astype('category')\n",
    "\n",
    "# Convert engineered features to float\n",
    "engineered_cols = ['feature_formula'] + \\\n",
    "                  [f'log_{col}' for col in num_features] + \\\n",
    "                  [f'{col}_sq' for col in num_features] + \\\n",
    "                  ['study_attendance_interaction', 'study_sleep_interaction', \n",
    "                   'attendance_sleep_interaction', 'study_sleep_ratio', 'attendance_sleep_ratio']\n",
    "\n",
    "for col in engineered_cols:\n",
    "    if col in full_data.columns:\n",
    "        full_data[col] = full_data[col].astype(float)\n",
    "\n",
    "# Split back\n",
    "X = full_data.iloc[:len(train_df)].copy()\n",
    "X_test = full_data.iloc[len(train_df):].copy()\n",
    "\n",
    "print(f\"Training features: {X.shape}\")\n",
    "print(f\"Test features: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost parameters (optimized from top notebooks)\n",
    "xgb_params = {\n",
    "    'n_estimators': 10000,\n",
    "    'learning_rate': 0.007,\n",
    "    'max_depth': 7,\n",
    "    'subsample': 0.8,\n",
    "    'num_parallel_tree': 2,\n",
    "    'reg_lambda': 3,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'colsample_bynode': 0.7,\n",
    "    'tree_method': 'hist',\n",
    "    'random_state': 42,\n",
    "    'early_stopping_rounds': 100,\n",
    "    'eval_metric': 'rmse',\n",
    "    'enable_categorical': True\n",
    "}\n",
    "\n",
    "# Cross-validation\n",
    "test_predictions_xgb = []\n",
    "oof_predictions_xgb = np.zeros(len(X))\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Training XGBoost with 5-fold CV...\\n\")\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X, y)):\n",
    "    print(f\"--- Fold {fold+1} ---\")\n",
    "    \n",
    "    X_train_fold, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train_fold, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    X_train_combined = X_train_fold\n",
    "    y_train_combined = y_train_fold\n",
    "    \n",
    "    model = xgb.XGBRegressor(**xgb_params)\n",
    "    \n",
    "    model.fit(\n",
    "        X_train_combined,\n",
    "        y_train_combined,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=200\n",
    "    )\n",
    "    \n",
    "    val_preds = model.predict(X_val)\n",
    "    oof_predictions_xgb[val_index] = val_preds\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "    print(f\"Fold {fold+1} RMSE: {rmse:.5f}\\n\")\n",
    "    \n",
    "    test_preds = model.predict(X_test)\n",
    "    test_predictions_xgb.append(test_preds)\n",
    "\n",
    "oof_rmse_xgb = np.sqrt(mean_squared_error(y, oof_predictions_xgb))\n",
    "print(f\"XGBoost OOF RMSE: {oof_rmse_xgb:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competition Information\n",
    "\n",
    "**Playground Series - Season 6, Episode 1: Predicting Student Test Scores**\n",
    "\n",
    "- **Evaluation Metric**: Root Mean Squared Error (RMSE)\n",
    "- **Target**: Predict `exam_score` for each student in the test set\n",
    "- **Best Public LB Scores**: ~8.56-8.64 RMSE\n",
    "- **Timeline**: January 1-31, 2026\n",
    "\n",
    "### Key Insights from Top Notebooks:\n",
    "1. **Feature Engineering is Critical**:\n",
    "   - Log transforms on numerical features\n",
    "   - Squared features\n",
    "   - Learned formula feature (linear combination of study_hours, class_attendance, sleep_hours)\n",
    "   - Interaction features (study×attendance, study×sleep, attendance×sleep)\n",
    "   - Ratio features (study/sleep, attendance/sleep)\n",
    "\n",
    "2. **Model Configuration**:\n",
    "   - XGBoost with categorical support (`enable_categorical=True`)\n",
    "   - 5-fold cross-validation\n",
    "   - Learning rate: 0.007\n",
    "   - Max depth: 7\n",
    "   - Early stopping: 100 rounds\n",
    "   - Many estimators (10000) with early stopping\n",
    "\n",
    "3. **Performance Expectations**:\n",
    "   - OOF RMSE: ~8.64-8.74 (without original data)\n",
    "   - Public LB: ~8.56-8.64 (with ensemble/blending)\n",
    "   - Training time: ~10-15 minutes per fold on Kaggle GPU\n",
    "\n",
    "---\n",
    "\n",
    "**⚠️ Important: Make sure to run all cells in order (0 → 1 → 2 → 3 → 4) before running the submission cells (6, 7)!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission\n",
    "# Make sure Cell 4 (model training) has been executed first!\n",
    "try:\n",
    "    # Try to access test_predictions_xgb\n",
    "    _ = test_predictions_xgb\n",
    "    if len(test_predictions_xgb) == 0:\n",
    "        raise ValueError(\"test_predictions_xgb is empty!\")\n",
    "except NameError:\n",
    "    raise NameError(\n",
    "        \"❌ ERROR: test_predictions_xgb not found!\\n\"\n",
    "        \"Please run Cell 4 (XGBoost model training) first.\\n\"\n",
    "        \"The training cell creates the predictions needed for submission.\\n\"\n",
    "        \"Execution order: Cell 0 → Cell 1 → Cell 2 → Cell 3 → Cell 4 → Cell 6\"\n",
    "    )\n",
    "\n",
    "submission_df[TARGET] = np.mean(test_predictions_xgb, axis=0)\n",
    "\n",
    "# Clip predictions to valid range (0-100 for exam scores)\n",
    "submission_df[TARGET] = np.clip(submission_df[TARGET], 0, 100)\n",
    "\n",
    "# Save submission\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(f\"\\nSubmission saved! Shape: {submission_df.shape}\")\n",
    "print(f\"Prediction range: [{submission_df[TARGET].min():.2f}, {submission_df[TARGET].max():.2f}]\")\n",
    "print(f\"Mean prediction: {submission_df[TARGET].mean():.2f}\")\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save OOF predictions for analysis\n",
    "# Make sure Cell 4 (model training) has been executed first!\n",
    "try:\n",
    "    # Try to access oof_predictions_xgb\n",
    "    _ = oof_predictions_xgb\n",
    "except NameError:\n",
    "    raise NameError(\n",
    "        \"❌ ERROR: oof_predictions_xgb not found!\\n\"\n",
    "        \"Please run Cell 4 (XGBoost model training) first.\\n\"\n",
    "        \"The training cell creates the OOF predictions needed for analysis.\\n\"\n",
    "        \"Execution order: Cell 0 → Cell 1 → Cell 2 → Cell 3 → Cell 4 → Cell 7\"\n",
    "    )\n",
    "\n",
    "oof_df = pd.DataFrame({\n",
    "    'id': train_df['id'],\n",
    "    TARGET: oof_predictions_xgb\n",
    "})\n",
    "oof_df.to_csv('oof_predictions.csv', index=False)\n",
    "print(\"OOF predictions saved to oof_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Using Original Dataset\n",
    "\n",
    "If you have access to the original dataset (`/kaggle/input/exam-score-prediction-dataset/Exam_Score_Prediction.csv`), you can improve performance by ~0.1-0.2 RMSE by including it in training:\n",
    "\n",
    "```python\n",
    "# Uncomment to use original dataset\n",
    "# original_df = pd.read_csv('/kaggle/input/exam-score-prediction-dataset/Exam_Score_Prediction.csv')\n",
    "# X_orig_raw = preprocess(original_df)\n",
    "# y_orig = original_df[TARGET].reset_index(drop=True)\n",
    "# \n",
    "# # Add to full_data for categorical encoding\n",
    "# full_data = pd.concat([X_raw, X_test_raw, X_orig_raw], axis=0)\n",
    "# # ... (rest of preprocessing)\n",
    "# \n",
    "# # In training loop:\n",
    "# X_train_combined = pd.concat([X_train_fold, X_original], axis=0)\n",
    "# y_train_combined = pd.concat([y_train_fold, y_orig], axis=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Summary\n",
    "\n",
    "- **Cross-Validation**: 5-fold KFold\n",
    "- **Expected OOF RMSE**: ~8.64-8.74 (without original data)\n",
    "- **Expected Public LB**: ~8.65-8.75 (single model)\n",
    "- **Training Time**: ~10-15 minutes per fold (on Kaggle GPU)\n",
    "\n",
    "### Feature Importance\n",
    "The model uses:\n",
    "- **Base features**: age, gender, course, study_hours, class_attendance, internet_access, sleep_hours, sleep_quality, study_method, facility_rating, exam_difficulty\n",
    "- **Engineered features**: \n",
    "  - Log transforms (log_study_hours, log_class_attendance, log_sleep_hours)\n",
    "  - Squared features (study_hours_sq, class_attendance_sq, sleep_hours_sq)\n",
    "  - Learned formula feature (linear combination)\n",
    "  - Interaction features (study×attendance, study×sleep, attendance×sleep)\n",
    "  - Ratio features (study/sleep, attendance/sleep)\n",
    "\n",
    "### Next Steps for Improvement:\n",
    "1. Use original dataset if available\n",
    "2. Try ensemble with LightGBM or CatBoost\n",
    "3. Hyperparameter tuning\n",
    "4. Feature selection\n",
    "5. Blending with other top solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- This notebook is ready to run on Kaggle\n",
    "- Make sure to enable GPU acceleration in Kaggle settings for faster training\n",
    "- The model uses XGBoost with categorical feature support\n",
    "- All predictions are clipped to [0, 100] range for valid exam scores\n",
    "- Submission file will be saved as `submission.csv`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
