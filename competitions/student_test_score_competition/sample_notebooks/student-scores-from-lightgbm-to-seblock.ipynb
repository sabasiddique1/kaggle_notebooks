{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":119082,"databundleVersionId":14993753,"sourceType":"competition"},{"sourceId":13904981,"sourceType":"datasetVersion","datasetId":8762382}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"V1-V22 uses the lightgbm model, with the best version being V11.\n\nFrom V23Ôºå try other models","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# ==============================\n# Ëá™ÂÆö‰πâ L4 ÊçüÂ§±ÂáΩÊï∞\n# ==============================\nclass L4Loss(nn.Module):\n    def __init__(self, reduction='mean'):\n        super().__init__()\n        self.reduction = reduction\n\n    def forward(self, y_pred, y_true):\n        diff = y_pred - y_true\n        loss = diff ** 4\n        if self.reduction == 'mean':\n            return loss.mean()\n        elif self.reduction == 'sum':\n            return loss.sum()\n        else:\n            return loss\n\n# ==============================\n# ËÆæÂ§áËÆæÁΩÆ\n# ==============================\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ==============================\n# Êï∞ÊçÆÂä†ËΩΩ\n# ==============================\ntrain_file = \"/kaggle/input/playground-series-s6e1/train.csv\"\ntest_file = \"/kaggle/input/playground-series-s6e1/test.csv\"\noriginal_file = \"/kaggle/input/exam-score-prediction-dataset/Exam_Score_Prediction.csv\"\n\ntrain_df = pd.read_csv(train_file)\ntest_df = pd.read_csv(test_file)\noriginal_df = pd.read_csv(original_file) \nsubmission_df = pd.read_csv(\"/kaggle/input/playground-series-s6e1/sample_submission.csv\") \nTARGET = 'exam_score'\n\nnum_features = ['study_hours', 'class_attendance', 'sleep_hours']\nbase_features = [col for col in train_df.columns if col not in [TARGET, 'id']]\nCATS = base_features\nNUMS = num_features  # only these are truly numerical\n\n# ==============================\n# ÁâπÂæÅÂ∑•Á®ãÔºà‰ªÖÂØπÊï∞ÂÄºÁâπÂæÅÊìç‰ΩúÔºâ\n# ==============================\ndef add_engineered_features(df):\n    df_temp = df.copy()\n    # Sine features\n    df_temp['_study_hours_sin'] = np.sin(2 * np.pi * df_temp['study_hours'] / 12).astype('float32')\n    df_temp['_class_attendance_sin'] = np.sin(2 * np.pi * df_temp['class_attendance'] / 12).astype('float32')\n\n    for col in num_features:\n        if col in df_temp.columns:\n            df_temp[f'log_{col}'] = np.log1p(df_temp[col])\n            df_temp[f'{col}_sq'] = df_temp[col] ** 2\n\n    # Linear combo feature\n    df_temp['feature_formula'] = (\n            5.9051154511950499 * df_temp['study_hours'] +\n            0.34540967058057986 * df_temp['class_attendance'] +\n            1.423461171860262 * df_temp['sleep_hours'] + 4.7819\n    )\n\n    # Keep categorical as string for encoding\n    for col in CATS:\n        df_temp[col] = df_temp[col].astype(str)\n\n    return df_temp\n\n\n# ==============================\n# ÂàÜÂà´È¢ÑÂ§ÑÁêÜÊï∞ÂÄºÂíåÁ±ªÂà´ÁâπÂæÅ\n# ==============================\ntrain_eng = add_engineered_features(train_df)\n\n# ÊâÄÊúâÊï∞ÂÄºÂàóÔºàÂåÖÊã¨ engineeredÔºâ\nall_num_cols = [col for col in train_eng.columns if col not in CATS + [TARGET, 'id']]\nall_cat_cols = CATS\n\n# Scaler for numerical\nscaler = StandardScaler()\nscaler.fit(train_eng[all_num_cols])\n\n# Encoder for categorical\nencoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\nencoder.fit(train_eng[all_cat_cols])\n\n\ndef preprocess_pipeline_separate(df):\n    df_eng = add_engineered_features(df)\n    # Numerical: scale\n    nums_scaled = scaler.transform(df_eng[all_num_cols])\n    # Categorical: encode to integers\n    cats_encoded = encoder.transform(df_eng[all_cat_cols]).astype(np.int64)\n    return nums_scaled, cats_encoded\n\n\nX_num, X_cat = preprocess_pipeline_separate(train_df)\ny = train_df[TARGET].values\nX_test_num, X_test_cat = preprocess_pipeline_separate(test_df)\nX_orig_num, X_orig_cat = preprocess_pipeline_separate(original_df)\ny_original = original_df[TARGET].values\n\n# ==============================\n# Ëé∑ÂèñÁ±ªÂà´ÁâπÂæÅÁöÑÂîØ‰∏ÄÂÄºÊï∞ÈáèÔºàÁî®‰∫é EmbeddingÔºâ\n# ==============================\ncat_unique_counts = []\nfor i, col in enumerate(all_cat_cols):\n    n_unique = int(encoder.categories_[i].size)\n    cat_unique_counts.append(n_unique)\n\nprint(\"Categorical feature cardinalities:\", cat_unique_counts)\n\n\n# ==============================\n# SE Block (Squeeze-and-Excitation)\n# ==============================\nclass SEBlock(nn.Module):\n    def __init__(self, channels, reduction=4):\n        super().__init__()\n        self.fc1 = nn.Linear(channels, channels // reduction, bias=False)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(channels // reduction, channels, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # x: (batch, channels)\n        se = x.mean(dim=0, keepdim=True)  # global avg pool -> (1, channels)\n        se = self.fc1(se)\n        se = self.relu(se)\n        se = self.fc2(se)\n        se = self.sigmoid(se)\n        return x * se  # broadcast\n\n\n# ==============================\n# Residual Block with SE\n# ==============================\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim, dropout=0.1, reduction=4):\n        super().__init__()\n        self.linear1 = nn.Linear(dim, dim)\n        self.linear2 = nn.Linear(dim, dim)\n        self.dropout = nn.Dropout(dropout)\n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n        self.se = SEBlock(dim, reduction=reduction)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        residual = x\n        # First sub-block\n        out = self.norm1(x)\n        out = self.linear1(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        # Second sub-block\n        out = self.norm2(out)\n        out = self.linear2(out)\n        out = self.dropout(out)\n        # SE\n        out = self.se(out)\n        # Residual connection\n        out = out + residual\n        return out\n\n\n# ==============================\n# ÂÆåÊï¥Ê®°ÂûãÔºöEmbedding + Concat + ResNet + Head\n# ==============================\nclass TabularResNetWithEmbedding(nn.Module):\n    def __init__(\n            self,\n            num_numerical,\n            cat_unique_counts,\n            embedding_dim=8,\n            hidden_dim=256,\n            n_blocks=4,\n            dropout=0.1,\n            head_dims=[64, 16]\n    ):\n        super().__init__()\n        self.num_numerical = num_numerical\n        self.embedding_dim = embedding_dim\n\n        # Embedding layers for each categorical feature\n        self.embeddings = nn.ModuleList([\n            nn.Embedding(n_cat + 1, embedding_dim, padding_idx=-1)  # -1 mapped to last index\n            for n_cat in cat_unique_counts\n        ])\n\n        total_cat_dim = len(cat_unique_counts) * embedding_dim\n        input_dim = num_numerical + total_cat_dim\n\n        # Projection to hidden_dim\n        self.proj = nn.Linear(input_dim, hidden_dim)\n        self.dropout_in = nn.Dropout(dropout)\n\n        # Residual blocks\n        self.blocks = nn.Sequential(\n            *[ResidualBlock(hidden_dim, dropout=dropout) for _ in range(n_blocks)]\n        )\n\n        # Prediction head\n        layers = []\n        prev = hidden_dim\n        for h in head_dims:\n            layers.extend([\n                nn.Linear(prev, h),\n                nn.ReLU(),\n                nn.Dropout(dropout)\n            ])\n            prev = h\n        layers.append(nn.Linear(prev, 1))\n        self.head = nn.Sequential(*layers)\n\n    def forward(self, x_num, x_cat):\n        # x_num: (B, num_numerical)\n        # x_cat: (B, n_cats)\n        batch_size = x_num.size(0)\n\n        # Embed categorical features\n        x_embeds = []\n        for i, emb in enumerate(self.embeddings):\n            # x_cat[:, i] shape: (B,)\n            xi = x_cat[:, i]\n            # Handle -1 (unknown): map to last embedding index\n            xi = torch.where(xi == -1, torch.tensor(emb.num_embeddings - 1, device=xi.device), xi)\n            embed_i = emb(xi)  # (B, embedding_dim)\n            x_embeds.append(embed_i)\n\n        x_cat_emb = torch.cat(x_embeds, dim=1)  # (B, total_cat_dim)\n\n        # Concat numerical and embedded categorical\n        x = torch.cat([x_num, x_cat_emb], dim=1)  # (B, input_dim)\n\n        # Project to hidden space\n        x = self.proj(x)\n        x = self.dropout_in(x)\n\n        # Residual blocks\n        x = self.blocks(x)\n\n        # Prediction head\n        out = self.head(x).squeeze(1)\n        return out\n\n\n# ==============================\n# ËÆ≠ÁªÉÂáΩÊï∞Ôºà‰ΩøÁî® L4 LossÔºå‰ΩÜÈ™åËØÅÁî® RMSEÔºâ\n# ==============================\ndef train_model(model, train_loader, val_loader, epochs=200, lr=1e-3, weight_decay=1e-5, patience=20, factor=0.5,\n                min_lr=1e-6):\n    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=factor, patience=patience // 2, min_lr=min_lr\n    )\n    train_criterion = L4Loss()          # ‚Üê ËÆ≠ÁªÉÁî® L4\n    val_criterion = nn.MSELoss()        # ‚Üê È™åËØÅÁî® MSEÔºà‰∏∫‰∫ÜÊ≠£Á°ÆËÆ°ÁÆó RMSEÔºâ\n\n    best_val_rmse = float('inf')\n    patience_counter = 0\n    best_weights = None\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0.0\n        for xb_num, xb_cat, yb in train_loader:\n            xb_num, xb_cat, yb = xb_num.to(device), xb_cat.to(device), yb.to(device)\n            optimizer.zero_grad()\n            pred = model(xb_num, xb_cat)\n            loss = train_criterion(pred, yb)\n            loss.backward()\n            # üëá Ê¢ØÂ∫¶Ë£ÅÂâ™Èò≤Ê≠¢ÁàÜÁÇ∏ÔºàL4 Ê¢ØÂ∫¶Â§ßÔºÅÔºâ\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            train_loss += loss.item()\n\n        model.eval()\n        val_mse = 0.0\n        with torch.no_grad():\n            for xb_num, xb_cat, yb in val_loader:\n                xb_num, xb_cat, yb = xb_num.to(device), xb_cat.to(device), yb.to(device)\n                pred = model(xb_num, xb_cat)\n                mse_loss = val_criterion(pred, yb)\n                val_mse += mse_loss.item()\n        val_mse /= len(val_loader)\n        val_rmse = val_mse ** 0.25\n        scheduler.step(val_rmse)  # Ë∞ÉÂ∫¶Âô®Êåâ RMSE Èôç\n\n        if val_rmse < best_val_rmse:\n            best_val_rmse = val_rmse\n            patience_counter = 0\n            best_weights = model.state_dict()\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                break\n\n        if (epoch + 1) % 5 == 0:\n            print(f\"Epoch {epoch + 1}/{epochs} | Val RMSE: {val_rmse:.5f}\")\n\n    if best_weights is not None:\n        model.load_state_dict(best_weights)\n    return model, best_val_rmse\n\n\n# ==============================\n# K ÊäòËÆ≠ÁªÉ\n# ==============================\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\ntest_predictions = []\noof_predictions = np.zeros(len(y))\n\nprint(f\"Starting {n_splits}-fold CV with L4 Loss...\")\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_num, y)):\n    print(f\"\\n--- Fold {fold + 1}/{n_splits} ---\")\n\n    # Split\n    X_num_train, X_cat_train = X_num[train_idx], X_cat[train_idx]\n    y_train = y[train_idx]\n    X_num_val, X_cat_val = X_num[val_idx], X_cat[val_idx]\n    y_val = y[val_idx]\n\n    # Augment with original data\n    X_num_combined = np.vstack([X_num_train, X_orig_num])\n    X_cat_combined = np.vstack([X_cat_train, X_orig_cat])\n    y_combined = np.concatenate([y_train, y_original])\n\n    # Tensors\n    X_num_train_t = torch.tensor(X_num_combined, dtype=torch.float32)\n    X_cat_train_t = torch.tensor(X_cat_combined, dtype=torch.int64)\n    y_train_t = torch.tensor(y_combined, dtype=torch.float32)\n\n    X_num_val_t = torch.tensor(X_num_val, dtype=torch.float32)\n    X_cat_val_t = torch.tensor(X_cat_val, dtype=torch.int64)\n    y_val_t = torch.tensor(y_val, dtype=torch.float32)\n\n    X_test_num_t = torch.tensor(X_test_num, dtype=torch.float32)\n    X_test_cat_t = torch.tensor(X_test_cat, dtype=torch.int64)\n\n    # Datasets & Loaders\n    train_ds = TensorDataset(X_num_train_t, X_cat_train_t, y_train_t)\n    val_ds = TensorDataset(X_num_val_t, X_cat_val_t, y_val_t)\n    train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=1024, shuffle=False)\n\n    # Model\n    model = TabularResNetWithEmbedding(\n        num_numerical=X_num.shape[1],\n        cat_unique_counts=cat_unique_counts,\n        embedding_dim=8,\n        hidden_dim=256,\n        n_blocks=3,\n        dropout=0.11,\n        head_dims=[64, 16]\n    ).to(device)\n\n    # Train\n    model, best_rmse = train_model(\n        model,\n        train_loader,\n        val_loader,\n        epochs=300,\n        lr=1e-3,\n        weight_decay=1e-4,\n        patience=20,\n        factor=0.5,\n        min_lr=1e-6\n    )\n\n    # Predict\n    model.eval()\n    with torch.no_grad():\n        val_pred = model(X_num_val_t.to(device), X_cat_val_t.to(device)).cpu().numpy()\n        test_pred = model(X_test_num_t.to(device), X_test_cat_t.to(device)).cpu().numpy()\n\n    oof_predictions[val_idx] = val_pred\n    test_predictions.append(test_pred)\n\n    print(f\"Fold {fold + 1} RMSE: {best_rmse:.5f}\")\n\n# ==============================\n# Final Evaluation & Submission\n# ==============================\noof_rmse = np.sqrt(mean_squared_error(y, oof_predictions))\nprint(\"\\n\" + \"=\" * 50)\nprint(f\"Final OOF RMSE: {oof_rmse:.6f}\")\nprint(\"=\" * 50)\n\noof_df = pd.DataFrame({'id': train_df['id'], TARGET: oof_predictions})\noof_df.to_csv('nn_oof.csv', index=False)\n\nsubmission_df[TARGET] = np.mean(test_predictions, axis=0)\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"\\nSubmission saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T13:19:57.60078Z","iopub.execute_input":"2026-01-04T13:19:57.601108Z"}},"outputs":[],"execution_count":null}]}