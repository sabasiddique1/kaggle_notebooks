{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92348b21",
   "metadata": {},
   "source": [
    "# Deep Learning Module Project — UCI Forest CoverType (Covertype)\n",
    "\n",
    "**Dataset:** UCI Forest CoverType via `sklearn.datasets.fetch_covtype`  \n",
    "**Objective:** Improve a baseline MLP (architecture + regularization + optimization), evaluate with robust metrics (class imbalance), compare against an ensemble model, and reflect on the results.\n",
    "\n",
    "> **Target:** ~94% test accuracy with proper tuning (as per the project brief). Exact results can vary slightly based on random seeds and hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15d8594",
   "metadata": {},
   "source": [
    "## Deliverable checklist (mapped to the brief)\n",
    "\n",
    "This notebook includes:\n",
    "\n",
    "- ✅ Modified **MLP architecture** (deeper/wider), tried multiple **activations**  \n",
    "- ✅ **Batch Normalization** with design justification  \n",
    "- ✅ Regularization: **Dropout** + **L2 (weight decay)**  \n",
    "- ✅ Optimizers: **Adam / RMSprop / SGD+momentum** + **ReduceLROnPlateau**  \n",
    "- ✅ Training management: **EarlyStopping**, epoch-wise logs, learning curves  \n",
    "- ✅ Evaluation: **Accuracy**, **Precision/Recall/F1 (macro & weighted)**, **Confusion Matrix**  \n",
    "- ✅ Comparison: **RandomForestClassifier** (same metrics)  \n",
    "- ✅ Reflection (3–5 lines) on why ensembles often outperform MLPs on tabular data\n",
    "\n",
    "---\n",
    "\n",
    "⚠️ **Note for this environment:** `fetch_covtype()` downloads the dataset if it isn't cached.  \n",
    "\n",
    "If you're running on **Kaggle/Colab/local with internet**, it will work normally and produce outputs + plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd9d1835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: /opt/homebrew/opt/python@3.10/bin/python3.10\n",
      "Python version: sys.version_info(major=3, minor=10, micro=18, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1cada0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d437ca23",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Core\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# Core\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "# Baseline ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Deep learning (Keras / TensorFlow)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dfdfbd",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f6983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# According to the project: Dataset is UCI Forest CoverType via sklearn.datasets.fetch_covtype\n",
    "# This function automatically downloads from UCI ML Repository if not cached\n",
    "\n",
    "import os\n",
    "from urllib.error import HTTPError, URLError\n",
    "\n",
    "print(\"Attempting to load UCI Forest CoverType dataset...\")\n",
    "print(\"Source: sklearn.datasets.fetch_covtype (downloads from UCI ML Repository)\")\n",
    "\n",
    "try:\n",
    "    # Method 1: Try sklearn's fetch_covtype (standard method per project requirements)\n",
    "    print(\"\\n[Method 1] Trying sklearn.datasets.fetch_covtype()...\")\n",
    "    data = fetch_covtype(download_if_missing=True)\n",
    "    X = data.data\n",
    "    y = data.target  # labels are 1..7\n",
    "    print(\"✓ Successfully loaded via sklearn.fetch_covtype()\")\n",
    "    \n",
    "except (HTTPError, URLError, Exception) as e:\n",
    "    print(f\"❌ sklearn download failed: {type(e).__name__}: {e}\")\n",
    "    print(\"\\n[Method 2] Attempting direct download from UCI ML Repository...\")\n",
    "    \n",
    "    try:\n",
    "        import urllib.request\n",
    "        import gzip\n",
    "        \n",
    "        # UCI ML Repository direct URL (as per project: UCI Forest CoverType)\n",
    "        url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\"\n",
    "        \n",
    "        # Use sklearn's default cache directory\n",
    "        data_home = os.path.expanduser('~/scikit_learn_data')\n",
    "        data_dir = os.path.join(data_home, 'covtype')\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "        data_file = os.path.join(data_dir, 'covtype.data.gz')\n",
    "        \n",
    "        if not os.path.exists(data_file):\n",
    "            print(f\"Downloading from: {url}\")\n",
    "            print(\"This may take a few minutes (dataset is ~11 MB)...\")\n",
    "            urllib.request.urlretrieve(url, data_file)\n",
    "            print(\"✓ Download complete\")\n",
    "        else:\n",
    "            print(f\"✓ Using cached file: {data_file}\")\n",
    "        \n",
    "        # Load the data (UCI format: CSV with last column as target)\n",
    "        print(\"Loading data from file...\")\n",
    "        with gzip.open(data_file, 'rb') as f:\n",
    "            X = np.genfromtxt(f, delimiter=',')\n",
    "        \n",
    "        # Last column is the target (classes 1-7)\n",
    "        y = X[:, -1].astype(int)\n",
    "        X = X[:, :-1]  # Features are all columns except last\n",
    "        \n",
    "        print(\"✓ Successfully loaded from UCI repository\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"❌ Direct download also failed: {type(e2).__name__}: {e2}\")\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"MANUAL DOWNLOAD INSTRUCTIONS:\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"1. Visit UCI ML Repository:\")\n",
    "        print(\"   https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/\")\n",
    "        print(\"\\n2. Download file: 'covtype.data.gz'\")\n",
    "        print(\"\\n3. Create directory and place file:\")\n",
    "        print(f\"   mkdir -p {os.path.expanduser('~/scikit_learn_data/covtype')}\")\n",
    "        print(f\"   # Then move covtype.data.gz to: {os.path.expanduser('~/scikit_learn_data/covtype/')}\")\n",
    "        print(\"\\n4. Re-run this cell\")\n",
    "        print(\"=\"*70)\n",
    "        raise\n",
    "\n",
    "# Verify dataset loaded correctly\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATASET LOADED SUCCESSFULLY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Number of samples: {X.shape[0]}\")\n",
    "print(f\"\\nClasses: {np.unique(y)}\")\n",
    "print(f\"Class counts: {np.bincount(y)[1:]}\")  # ignore index 0 (unused)\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4b9b7c",
   "metadata": {},
   "source": [
    "## Data split + preprocessing\n",
    "\n",
    "The dataset contains:\n",
    "- **10 continuous** features (elevation, slope, distances, etc.)\n",
    "- **44 binary one-hot** indicator columns (wilderness area + soil type)\n",
    "\n",
    "A practical preprocessing choice:\n",
    "- **Standardize** only the **first 10 continuous** columns\n",
    "- Keep the binary one-hot columns as-is (they're already 0/1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eae08bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to 0..6 for Keras\n",
    "y = y - 1\n",
    "\n",
    "# Train/Val/Test split (stratified)\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=0.2, random_state=42, stratify=y_trainval\n",
    ")\n",
    "\n",
    "# Scale only the first 10 continuous columns\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_val_scaled = X_val.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "X_train_scaled[:, :10] = scaler.fit_transform(X_train[:, :10])\n",
    "X_val_scaled[:, :10] = scaler.transform(X_val[:, :10])\n",
    "X_test_scaled[:, :10] = scaler.transform(X_test[:, :10])\n",
    "\n",
    "print(\"Train/Val/Test shapes:\", X_train_scaled.shape, X_val_scaled.shape, X_test_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc7221c",
   "metadata": {},
   "source": [
    "## 3.1 Neural Network Architecture\n",
    "\n",
    "We improve the baseline MLP by:\n",
    "- Going **deeper/wider** (e.g., 512 → 256 → 128)\n",
    "- Trying different activations (ReLU, LeakyReLU, SELU)\n",
    "- Adding **BatchNorm** after Dense layers to stabilize training & help higher learning rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a60b65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X_train_scaled.shape[1]\n",
    "num_classes = 7\n",
    "\n",
    "def build_mlp(\n",
    "    hidden_sizes=(512, 256, 128),\n",
    "    activation=\"relu\",\n",
    "    dropout=0.25,\n",
    "    l2=2e-4,\n",
    "    lr=1e-3,\n",
    "):\n",
    "    inputs = keras.Input(shape=(num_features,), name=\"features\")\n",
    "    x = inputs\n",
    "\n",
    "    for h in hidden_sizes:\n",
    "        x = layers.Dense(h, kernel_regularizer=regularizers.l2(l2))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        if activation == \"leaky_relu\":\n",
    "            x = layers.LeakyReLU(alpha=0.01)(x)\n",
    "        else:\n",
    "            x = layers.Activation(activation)(x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"class\")(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"MLP_Covertype_Tuned\")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "mlp = build_mlp()\n",
    "mlp.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5326cbbb",
   "metadata": {},
   "source": [
    "## 3.2 Regularization Techniques\n",
    "\n",
    "We use:\n",
    "- **Dropout** to reduce co-adaptation and overfitting\n",
    "- **L2 regularization** on Dense kernels (`kernel_regularizer=l2(...)`)\n",
    "\n",
    "Tune these:\n",
    "- Dropout: 0.10 → 0.40  \n",
    "- L2: 1e-5 → 5e-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c744a1dd",
   "metadata": {},
   "source": [
    "## 3.3 Optimizer and Learning Rate Strategy\n",
    "\n",
    "Try multiple optimizers:\n",
    "- Adam (good default)\n",
    "- RMSprop (sometimes smoother on some problems)\n",
    "- SGD + Momentum (often best with careful LR scheduling)\n",
    "\n",
    "We also use:\n",
    "- **ReduceLROnPlateau** to lower LR when validation stops improving\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b51e0d5",
   "metadata": {},
   "source": [
    "## 3.4 Training Management\n",
    "\n",
    "We integrate:\n",
    "- **EarlyStopping** to stop training when validation accuracy plateaus\n",
    "- Logging per epoch via `history`\n",
    "- Learning curve visualization (accuracy + loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72bd23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\", patience=6, restore_best_weights=True\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5, verbose=1\n",
    "    ),\n",
    "]\n",
    "\n",
    "history = mlp.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=4096,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec9bfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves\n",
    "plt.figure()\n",
    "plt.plot(history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(\"MLP Accuracy\")\n",
    "plt.legend(); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"MLP Loss\")\n",
    "plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25806d81",
   "metadata": {},
   "source": [
    "## 3.5 Model Evaluation\n",
    "\n",
    "Because Forest CoverType is imbalanced, accuracy alone can mislead. We report:\n",
    "- Accuracy\n",
    "- Precision/Recall/F1 (**macro** + **weighted**)\n",
    "- Confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332c642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "test_probs = mlp.predict(X_test_scaled, batch_size=4096, verbose=0)\n",
    "y_pred = np.argmax(test_probs, axis=1)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"MLP Test Accuracy:\", acc)\n",
    "\n",
    "# Macro + weighted PRF\n",
    "prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "    y_test, y_pred, average=\"macro\"\n",
    ")\n",
    "prec_w, rec_w, f1_w, _ = precision_recall_fscore_support(\n",
    "    y_test, y_pred, average=\"weighted\"\n",
    ")\n",
    "\n",
    "print(f\"Macro   P/R/F1: {prec_macro:.4f} / {rec_macro:.4f} / {f1_macro:.4f}\")\n",
    "print(f\"Weighted P/R/F1: {prec_w:.4f} / {rec_w:.4f} / {f1_w:.4f}\")\n",
    "\n",
    "print(\"\\nClassification report (MLP):\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f30d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix plot (MLP)\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(cm, interpolation=\"nearest\")\n",
    "plt.title(\"MLP Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.colorbar()\n",
    "\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\", fontsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4e7d43",
   "metadata": {},
   "source": [
    "## 3.6 Comparison with Ensemble Method (RandomForest)\n",
    "\n",
    "We train a RandomForest on the **same dataset split** and report the same metrics for fair comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5583fee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_features=\"sqrt\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "rf.fit(X_trainval, y_trainval)\n",
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "rf_acc = accuracy_score(y_test, rf_pred)\n",
    "print(\"RF Test Accuracy:\", rf_acc)\n",
    "\n",
    "prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "    y_test, rf_pred, average=\"macro\"\n",
    ")\n",
    "prec_w, rec_w, f1_w, _ = precision_recall_fscore_support(\n",
    "    y_test, rf_pred, average=\"weighted\"\n",
    ")\n",
    "\n",
    "print(f\"Macro   P/R/F1: {prec_macro:.4f} / {rec_macro:.4f} / {f1_macro:.4f}\")\n",
    "print(f\"Weighted P/R/F1: {prec_w:.4f} / {rec_w:.4f} / {f1_w:.4f}\")\n",
    "\n",
    "print(\"\\nClassification report (RF):\")\n",
    "print(classification_report(y_test, rf_pred, digits=4))\n",
    "\n",
    "rf_cm = confusion_matrix(y_test, rf_pred)\n",
    "rf_cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa92c903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix plot (RandomForest)\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(rf_cm, interpolation=\"nearest\")\n",
    "plt.title(\"RandomForest Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.colorbar()\n",
    "\n",
    "for i in range(rf_cm.shape[0]):\n",
    "    for j in range(rf_cm.shape[1]):\n",
    "        plt.text(j, i, str(rf_cm[i, j]), ha=\"center\", va=\"center\", fontsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1a87dd",
   "metadata": {},
   "source": [
    "## Reflection (3–5 lines)\n",
    "\n",
    "Tree-based ensembles often outperform MLPs on structured/tabular datasets because they naturally model **nonlinear feature interactions** and **piecewise decision boundaries** with minimal preprocessing. They also handle **mixed feature types** and sparse one-hot indicators robustly, and are less sensitive to scaling and learning-rate dynamics. In contrast, MLPs usually need careful tuning (architecture, regularization, LR schedules) to match ensemble performance on tabular data, even when they do well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc67f4c1",
   "metadata": {},
   "source": [
    "## Tuning notes (how to chase ~94%)\n",
    "\n",
    "If you are below the target accuracy, iterate using a simple grid:\n",
    "- **Hidden sizes:** (1024, 512, 256) vs (512, 256, 128)  \n",
    "- **Activation:** ReLU vs LeakyReLU vs SELU (with AlphaDropout)  \n",
    "- **Dropout:** 0.10 → 0.35  \n",
    "- **L2:** 1e-5 → 5e-4  \n",
    "- **Optimizers:** Adam vs RMSprop vs SGD(momentum=0.9)  \n",
    "- **Batch size:** 2048 / 4096 / 8192  \n",
    "\n",
    "Keep a short experiment log table (hyperparams → val acc → test metrics) so your process is clearly documented.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
